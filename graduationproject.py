# -*- coding: utf-8 -*-
"""LastProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sl5CisLs1ReQ1EC7usIH9OpV9ZPKHMyu
"""

import tensorflow as tf
from tensorflow.keras import models, layers
import matplotlib.pyplot as plt
from tensorflow.keras.applications import MobileNet,ResNet50, VGG16
from tensorflow.keras.applications.mobilenet import preprocess_input, decode_predictions
from tensorflow.keras.applications.resnet import preprocess_input, decode_predictions

dataset = tf.keras.preprocessing.image_dataset_from_directory(
    "/content/drive/My Drive/PlantVillage",
    shuffle=True,
    image_size=(256,256),
    batch_size=32
)

class_names = dataset.class_names
class_names

plt.figure(figsize=(10,10))
for image_batch, label_batch in dataset.take(1):
  for i in range(12):
    ax = plt.subplot(3,4,i+1)
    plt.imshow(image_batch[i].numpy().astype("uint8"))
    plt.title(class_names[label_batch[i]])
    plt.axis("off")

class_names = dataset.class_names

# Find the index of the "early blight" class
healthy_index = class_names.index("Potato___healthy")

# Initialize a list to store images of "early blight"
healthy_images = []

# Iterate over the dataset to collect images of "early blight"
for image_batch, label_batch in dataset:
    for image, label in zip(image_batch, label_batch):
        if label == healthy_index:
            healthy_images.append(image)
        if len(healthy_images) >= 12:  # Collect only 12 images
            break
    if len(healthy_images) >= 12:
        break

# Display the images
plt.figure(figsize=(10, 10))
for i in range(12):
    ax = plt.subplot(3, 4, i + 1)
    plt.imshow(healthy_images[i].numpy().astype("uint8"))
    plt.title("Potato___healthy")
    plt.axis("off")

plt.show()

class_names = dataset.class_names

# Find the index of the "early blight" class
early_blight_index = class_names.index("Potato___Late_blight")

# Initialize a counter for "early blight" images
early_blight_count = 0

# Iterate over the dataset to count occurrences of "early blight"
for image_batch, label_batch in dataset:
    early_blight_count += sum(label_batch.numpy() == early_blight_index)

# Print the information of the "early blight" class
print(f"Class 'late blight' information:")
print(f"Total number of images: {early_blight_count}")

len(dataset)

train_ds = dataset.take(48)
len(train_ds)

test_ds = dataset.skip(48)
len(test_ds)

val_ds = test_ds.take(10)
len(val_ds)

test_ds = test_ds.skip(10)
len(test_ds)

def get_dataset_partitions_tf(ds, train_split=0.7, val_split=0.15, test_split=0.15,shuffle=True,shuffle_size=10000):
    ds_size = len(ds)

    if shuffle:
      ds = ds.shuffle(shuffle_size, seed=12)

    train_size = int(train_split * ds_size)
    val_size = int(val_split * ds_size)

    train_ds = ds.take(train_size)

    val_ds = ds.skip(train_size).take(val_size)
    test_ds = ds.skip(train_size).skip(val_size)

    return train_ds, val_ds, test_ds

train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset)

train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)

resize_and_rescale = tf.keras.Sequential([
    layers.experimental.preprocessing.Resizing(256,256),
    layers.experimental.preprocessing.Rescaling(1.0/255)
])

data_augmentation = tf.keras.Sequential([
    layers.experimental.preprocessing.RandomFlip("horizontal_and_vertical"),
    layers.experimental.preprocessing.RandomRotation(0.2),
])

model = models.Sequential([
    resize_and_rescale,
    data_augmentation,
    layers.Conv2D(32, (3,3), activation='relu', input_shape=(256,256)),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(3, activation='softmax')
])

model.build(input_shape=(32, 256, 256, 3))

model.summary()

model.compile(
    optimizer='adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics=['accuracy'])

history = model.fit(
    train_ds,
    epochs=10,
    batch_size=128,
    verbose=1,
    validation_data=val_ds)

scores = model.evaluate(test_ds)

from keras.models import load_model

model.save('model.keras')

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

with open("model.tflite", 'wb') as f:
  f.write(tflite_model)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(10,10))
plt.subplot(1,2,1)
plt.plot(range(10), acc, label='Training Accuracy')
plt.plot(range(10), val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1,2,2)
plt.plot(range(10), loss, label='Training Loss')
plt.plot(range(10), val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')

plt.show()

import numpy as np
for images_batch, labels_batch in test_ds.take(1):

  first_image = image_batch[0].numpy().astype('uint8')
  first_label = label_batch[0].numpy()

  plt.imshow(first_image)
  print("Image's actual label: ",class_names[first_label])

  batch_prediction = model.predict(image_batch)
  print("Predicted label: ",class_names[np.argmax(batch_prediction[0])])

def predict(model, img):
  img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())
  img_array = tf.expand_dims(img_array, 0)

  predictions = model.predict(img_array)

  predicted_class = class_names[np.argmax(predictions[0])]
  confidence = round(100 * (np.max(predictions[0])), 2)
  return predicted_class, confidence

plt.figure(figsize=(15,15))
for images, labels in test_ds.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i+1)
    plt.imshow(images[i].numpy().astype("uint8"))

    predicted_class, confidence = predict(model, images[i].numpy())
    actual_class = class_names[labels[i]]

    plt.title(f"Actual: {actual_class}\n Predict: {predicted_class}")

    plt.axis("off")

"""MODEL MOBILENET"""

mobilenet_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

for layer in mobilenet_model.layers:
    layer.trainable = False

model_mobile_net = models.Sequential([
    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(256, 256, 3)),
    layers.experimental.preprocessing.Resizing(256, 256),  # Chỉnh kích thước đầu vào cho phù hợp với MobileNet
    mobilenet_model,
    layers.GlobalAveragePooling2D(),  # Lấy giá trị trung bình trên toàn bộ feature map
    layers.Dense(64, activation='relu'),
    layers.Dense(3, activation='softmax')
])

model_mobile_net.summary()

model_mobile_net.compile(optimizer='adam',
                         loss='sparse_categorical_crossentropy',
                         metrics=['accuracy'])

history_mobile_net = model_mobile_net.fit(
    train_ds,
    epochs=10,
    batch_size=128,
    verbose=1,
    validation_data=val_ds)

scores2 = model_mobile_net.evaluate(test_ds)

acc2 = history_mobile_net.history['accuracy']
val_acc2 = history_mobile_net.history['val_accuracy']

loss2 = history_mobile_net.history['loss']
val_loss2 = history_mobile_net.history['val_loss']

plt.figure(figsize=(10,10))
plt.subplot(1,2,1)
plt.plot(range(10), acc2, label='Training Accuracy')
plt.plot(range(10), val_acc2, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('MobileNet Training and Validation Accuracy')

plt.subplot(1,2,2)
plt.plot(range(10), loss2, label='Training Loss')
plt.plot(range(10), val_loss2, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')

plt.show()

"""MODEL RESNET"""

resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(256, 256, 3))

for layer in resnet_model.layers:
    layer.trainable = False

model_resnet = models.Sequential([
    layers.Input(shape=(256, 256, 3)),
    layers.experimental.preprocessing.Rescaling(1./255),  # Chuẩn hóa giá trị pixel về khoảng (0,1)
    layers.experimental.preprocessing.Resizing(256, 256),  # Chỉnh kích thước đầu vào cho phù hợp với ResNet50
    resnet_model,
    layers.GlobalAveragePooling2D(),  # Lấy giá trị trung bình trên toàn bộ feature map
    layers.Dense(64, activation='relu'),
    layers.Dense(3, activation='softmax')  # 3 đơn vị ở lớp đầu ra cho 3 lớp của tập dữ liệu
])

model_resnet.summary()

model_resnet.compile(optimizer='adam',
                     loss='sparse_categorical_crossentropy',
                     metrics=['accuracy'])

history_resnet = model_resnet.fit(
    train_ds,
    epochs=10,
    batch_size=128,
    verbose=1,
    validation_data=val_ds)

scores3 = model_resnet.evaluate(test_ds)

acc3 = history_resnet.history['accuracy']
val_acc3 = history_resnet.history['val_accuracy']

loss3 = history_resnet.history['loss']
val_loss3 = history_resnet.history['val_loss']

plt.figure(figsize=(10,10))
plt.subplot(1,2,1)
plt.plot(range(10), acc3, label='Training Accuracy')
plt.plot(range(10), val_acc3, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('MobileNet Training and Validation Accuracy')

plt.subplot(1,2,2)
plt.plot(range(10), loss3, label='Training Loss')
plt.plot(range(10), val_loss3, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')

plt.show()

"""MODEL VGG16"""

from tensorflow.keras import regularizers

vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

for layer in vgg_model.layers:
    layer.trainable = False

model_vgg = models.Sequential([
    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(256, 256, 3)),
    layers.experimental.preprocessing.Resizing(224, 224),
    vgg_model,
    layers.Dropout(0.5),
    layers.Flatten(),
    layers.Dense(4096, activation='relu',kernel_regularizer=regularizers.l2(0.01)),
    layers.Dense(4096, activation='relu'),
    layers.Dense(3, activation='softmax')
])

model_vgg.summary()

model_vgg.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

history_vgg = model_vgg.fit(train_ds,
                        validation_data=val_ds,
                        epochs=10,
                        batch_size=128,
                        verbose=1)

scores4 = model_vgg.evaluate(test_ds)

acc4 = history_vgg.history['accuracy']
val_acc4 = history_vgg.history['val_accuracy']

loss4 = history_vgg.history['loss']
val_loss4 = history_vgg.history['val_loss']

plt.figure(figsize=(10,10))
plt.subplot(1,2,1)
plt.plot(range(10), acc4, label='Training Accuracy')
plt.plot(range(10), val_acc4, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('MobileNet Training and Validation Accuracy')

plt.subplot(1,2,2)
plt.plot(range(10), loss4, label='Training Loss')
plt.plot(range(10), val_loss4, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')

plt.show()

model_path = '/content/drive/My Drive/model.pth'
model = YourModelClass()
model.load_state_dict(torch.load(model_path))

!pip install transformers

!pip install timm

import tensorflow as tf
import timm
import torch
import numpy as np

# Tải mô hình ViT từ timm
vit_model = timm.create_model('vit_base_patch16_224', pretrained=True)
vit_model.eval()  # Đặt chế độ eval để không cập nhật trọng số

# Đóng băng các lớp của ViT
for param in vit_model.parameters():
    param.requires_grad = False

# Lớp Keras tùy chỉnh để sử dụng mô hình PyTorch
class VitLayer(tf.keras.layers.Layer):
    def __init__(self, vit_model):
        super(VitLayer, self).__init__()
        self.vit_model = vit_model
        # Xác định phần cố định của đầu ra bằng cách sử dụng input giả
        dummy_input = torch.zeros(1, 3, 224, 224)
        with torch.no_grad():
            dummy_output = vit_model(dummy_input)
        self._output_shape = dummy_output.shape[1:]

    def call(self, inputs):
        def vit_predict(inputs):
            inputs = torch.tensor(inputs.numpy(), dtype=torch.float32)
            with torch.no_grad():
                outputs = self.vit_model(inputs.permute(0, 3, 1, 2))  # Chuyển từ NHWC sang NCHW
            return outputs.numpy()

        outputs = tf.py_function(vit_predict, [inputs], tf.float32)
        # Đặt hình dạng động cho batch size và cố định cho các chiều khác
        outputs.set_shape([None] + list(self._output_shape))
        return outputs

# Xây dựng mô hình
input_shape = (256, 256, 3)
model_vit = tf.keras.Sequential([
    tf.keras.layers.Input(shape=input_shape),
    tf.keras.layers.Rescaling(1./255),  # Chuẩn hóa giá trị pixel
    tf.keras.layers.Resizing(224, 224),  # Đảm bảo kích thước ảnh đầu vào
    VitLayer(vit_model),  # Sử dụng lớp ViT tùy chỉnh
    tf.keras.layers.Dense(128, activation='relu'),  # Thêm lớp Dense lớn
    tf.keras.layers.Dense(3, activation='softmax')  # Lớp Dense đầu ra với 3 lớp phân loại
])

model_vit.summary()

model_vit.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),  # Tăng learning rate
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

history_vit = model_vit.fit(
    train_ds,
    validation_data=val_ds,
    epochs=3,
    batch_size=128,
    verbose=1,
)

scores5 = model_vit.evaluate(test_ds)

acc5 = history_vit.history['accuracy']
val_acc5 = history_vit.history['val_accuracy']

loss5 = history_vit.history['loss']
val_loss5 = history_vit.history['val_loss']

plt.figure(figsize=(10,10))
plt.subplot(1,2,1)
plt.plot(range(10), acc5, label='Training Accuracy')
plt.plot(range(10), val_acc5, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('MobileNet Training and Validation Accuracy')

plt.subplot(1,2,2)
plt.plot(range(10), val_loss5, label='Training Loss')
plt.plot(range(10), val_loss5, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')

plt.show()